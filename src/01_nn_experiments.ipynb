{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN Experiments\n",
    "\n",
    "- Vanilla and Features NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "import time as time\n",
    "import sys\n",
    "import platform\n",
    "import psutil\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from captum.attr import Saliency\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from prox_op import prox_op\n",
    "from data_fcns import generate_raw_data\n",
    "from data_fcns import vanilla_scaling\n",
    "from data_fcns import compute_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python version\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get CPU info\n",
    "\n",
    "print(platform.processor())\n",
    "print(platform.machine())\n",
    "print(platform.version())\n",
    "print(platform.platform())\n",
    "print(platform.uname())\n",
    "print(platform.system())\n",
    "print(str(round(psutil.virtual_memory().total / (1024.0 **3)))+\" GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get GPU info\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.cuda.current_device())\n",
    "    print(torch.cuda.device(0))\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_type = \"feature\"  # vanilla or feature\n",
    "data_dist = \"unif\"   # norm, unif, or both\n",
    "unif_min = 0\n",
    "unif_max = 20\n",
    "min_len = 1000\n",
    "max_len = 2000\n",
    "num_vec = 10000\n",
    "seed = 1\n",
    "\n",
    "X, lengths, alphas, taus = generate_raw_data(data_dist, min_len, max_len, num_vec, unif_min, unif_max, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(lengths.shape)\n",
    "print(taus.shape)\n",
    "print(alphas.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nn_type == \"vanilla\":\n",
    "\n",
    "    num_moments = 0 # needed for the NN selection cell\n",
    "    M, yhat, zero_idx = vanilla_scaling(X, lengths, alphas, taus)\n",
    "    \n",
    "else:  # features NN\n",
    "    num_moments = 10\n",
    "    M, yhat, mus, zero_idx = compute_features(X, lengths, alphas, taus, num_moments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any observations from dataset that have tau = 0 \n",
    "\n",
    "if sum(zero_idx) > 0:\n",
    "    M = M[~zero_idx,:]\n",
    "    yhat = yhat[~zero_idx]\n",
    "    mus = mus[~zero_idx]\n",
    "    alphas = alphas[~zero_idx]\n",
    "    taus = taus[~zero_idx]\n",
    "    \n",
    "if data_dist == \"norm\":\n",
    "    num_norm_vec = M.shape[0]\n",
    "elif data_dist == \"unif\":  \n",
    "    num_norm_vec = 0\n",
    "else: # both\n",
    "    num_norm_vec = int(num_vec/2) - sum(np.where(zero_idx)[0] < int(num_vec/2))\n",
    "    \n",
    "print(num_norm_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TauDataset(Dataset):\n",
    "    def __init__(self, X, y, num_norm_vec):\n",
    "        \n",
    "        d = np.zeros(len(y))\n",
    "        d[:num_norm_vec] = 1  # Gaussian - 1, Uniform - 0\n",
    "          \n",
    "        self.features = torch.Tensor(X)\n",
    "        self.labels = torch.Tensor(y) \n",
    "        self.dist = torch.Tensor(d)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx,:], self.labels[idx], self.dist[idx], idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "dataset = TauDataset(M,yhat,num_norm_vec)\n",
    "\n",
    "print(dataset[0])\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_dist == \"both\":\n",
    "\n",
    "    # train-test split: 80-20, stratified sampling on distribution type\n",
    "    # Gaussian: 1st half of indices, Uniform: 2nd half of indices \n",
    "\n",
    "    train_idx, test_idx = train_test_split(range(len(dataset)), test_size=0.20, random_state=0, \n",
    "                                       shuffle=True, stratify=dataset.dist)\n",
    "    \n",
    "else: # unif or norm data\n",
    "    \n",
    "    # train-test split: 80-20\n",
    "    train_idx, test_idx = train_test_split(range(len(dataset)), test_size=0.20, random_state=0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(dataset.dist[train_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(dataset.dist[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Subset(dataset, train_idx)\n",
    "test_data = Subset(dataset, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dataset iterator\n",
    "\n",
    "train_batch_size = 32\n",
    "test_batch_size = len(test_data)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=train_batch_size, shuffle=True) \n",
    "test_loader = DataLoader(dataset=test_data, batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first layer number of inputs\n",
    "print(dataset[0][0].size())\n",
    "layer1_size = M[0].shape[0]\n",
    "layer1_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set NN based on layer1_size\n",
    "\n",
    "if layer1_size == (num_moments+3):  # features NN\n",
    "    \n",
    "    class NeuralNetwork(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(num_moments+3, 25),  \n",
    "                nn.ReLU(),    \n",
    "                nn.Linear(25, 10),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(10, 1)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            tau = self.linear_relu_stack(x) \n",
    "            return tau\n",
    "        \n",
    "elif (layer1_size == 2000) or (layer1_size == 100000):   # vanilla NN\n",
    "    \n",
    "    class NeuralNetwork(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(layer1_size, 200),   \n",
    "                nn.ReLU(),  \n",
    "                nn.Linear(200, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 50), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(50, 1) \n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            tau = self.linear_relu_stack(x)\n",
    "            return tau\n",
    "    \n",
    "else:\n",
    "    pass\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on CPU or GPU - use appropriate timing commands!\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    #size = len(dataloader.dataset)\n",
    "    \n",
    "    # GPU timers\n",
    "    #start_time = torch.cuda.Event(enable_timing=True)\n",
    "    #end_time = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss_per_obs = 0\n",
    "    epoch_time = 0\n",
    "    \n",
    "    for batch, (X, y, dist, idx) in enumerate(dataloader):\n",
    "        \n",
    "        # CPU time \n",
    "        t1 = time.perf_counter()\n",
    "        # GPU time\n",
    "        #start_time.record()\n",
    "           \n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(X)  \n",
    "        loss = loss_fn(pred, y.unsqueeze(1))  \n",
    "\n",
    "        # Backpropagation\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # CPU time\n",
    "        t2 = time.perf_counter()\n",
    "        epoch_time += t2-t1\n",
    "        # GPU time\n",
    "        #end_time.record()\n",
    "        #torch.cuda.synchronize()\n",
    "        #epoch_time += (start_time.elapsed_time(end_time))/1000  # time unit is milliseconds\n",
    "        \n",
    "        avg_loss_per_obs += loss.item()\n",
    "            \n",
    "    avg_loss_per_obs /= num_batches\n",
    "    print(f\"Epoch train time: {epoch_time} seconds\\n\")\n",
    "    print(f\"Train Error: \\n Avg loss (per obs): {avg_loss_per_obs:.2e} \\n\")   \n",
    "    \n",
    "    return avg_loss_per_obs, epoch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on CPU or GPU - use appropriate timing commands!\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    \n",
    "    #size = len(dataloader.dataset)\n",
    "    \n",
    "    # GPU timers\n",
    "    #start_time = torch.cuda.Event(enable_timing=True)\n",
    "    #end_time = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    num_batches = len(dataloader)\n",
    "   \n",
    "    model.eval()\n",
    "    test_loss_all = 0\n",
    "    test_loss_norm = 0\n",
    "    test_loss_unif = 0\n",
    "    test_loss_og = 0\n",
    "    epoch_time = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y, dist, idx in dataloader:\n",
    "            X, y, dist, idx = X.to(device), y.to(device), dist.to(device), idx.to(device)\n",
    "            \n",
    "            # CPU time\n",
    "            t1 = time.perf_counter()\n",
    "            # GPU time\n",
    "            #start_time.record()\n",
    "            \n",
    "            pred = model(X)\n",
    "            \n",
    "            # CPU time\n",
    "            t2 = time.perf_counter()\n",
    "            epoch_time += t2-t1\n",
    "            # GPU time\n",
    "            #end_time.record()\n",
    "            #torch.cuda.synchronize()\n",
    "            #epoch_time += (start_time.elapsed_time(end_time))/1000  # time unit is milliseconds\n",
    "            \n",
    "            test_loss_all += loss_fn(pred, y.unsqueeze(1)).item()\n",
    "            test_loss_norm += loss_fn(pred[dist == 1], y[dist == 1].unsqueeze(1)).item() \n",
    "            test_loss_unif += loss_fn(pred[dist == 0], y[dist == 0].unsqueeze(1)).item()\n",
    "    \n",
    "            # additional code - loss on original tau.  If use GPU - must send pred to CPU\n",
    "\n",
    "            if device == \"cuda\":\n",
    "                pred_og_tau = pred.squeeze().cpu().numpy()\n",
    "                idx = idx.cpu()\n",
    "            else:\n",
    "                pred_og_tau = pred.squeeze().numpy()\n",
    "        \n",
    "            if nn_type == \"feature\":\n",
    "\n",
    "                pred_og_tau = np.add(pred_og_tau, mus[idx])\n",
    "                pred_og_tau = np.multiply(alphas[idx], pred_og_tau)\n",
    "                test_loss_og += loss_fn(torch.Tensor(pred_og_tau), torch.Tensor(taus[idx])).item()     \n",
    "                    \n",
    "            else:  # vanilla NN\n",
    "                \n",
    "                pred_og_tau = np.multiply(alphas[idx], pred_og_tau)\n",
    "                test_loss_og += loss_fn(torch.Tensor(pred_og_tau), torch.Tensor(taus[idx])).item()             \n",
    "                \n",
    "    test_loss_all /= num_batches\n",
    "    test_loss_norm /= num_batches\n",
    "    test_loss_unif /= num_batches\n",
    "    test_loss_og /= num_batches\n",
    "    \n",
    "    print(f\"Epoch test time: {epoch_time} seconds\\n\")\n",
    "    print(f\"Test Error: \\n Avg loss (per obs): {test_loss_all:.2e} \\n\") \n",
    "    print(f\"Gaussian Test Error: \\n Avg loss (per obs): {test_loss_norm:.2e} \\n\")\n",
    "    print(f\"Uniform Test Error: \\n Avg loss (per obs): {test_loss_unif:.2e} \\n\")\n",
    "    print(f\"Original Tau Test Error: \\n Avg loss (per obs): {test_loss_og:.2e} \\n\")   \n",
    "\n",
    "    return test_loss_all, test_loss_norm, test_loss_unif, test_loss_og, epoch_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_time = np.zeros(epochs)\n",
    "test_time = np.zeros(epochs)\n",
    "\n",
    "avg_loss_per_obs_test = np.zeros(epochs)\n",
    "avg_loss_per_obs_train = np.zeros(epochs)\n",
    "\n",
    "avg_loss_per_obs_test_norm = np.zeros(epochs)\n",
    "avg_loss_per_obs_test_unif = np.zeros(epochs)\n",
    "avg_loss_per_obs_test_og = np.zeros(epochs)\n",
    "\n",
    "min_og_loss = 100\n",
    "min_epoch = 0\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    avg_loss_per_obs_train[t], train_time[t] = train(train_loader, model, loss_fn, optimizer)\n",
    "    avg_loss_per_obs_test[t], avg_loss_per_obs_test_norm[t], avg_loss_per_obs_test_unif[t], avg_loss_per_obs_test_og[t], test_time[t] = test(test_loader, model, loss_fn)\n",
    "\n",
    "    # save model - at epoch with minimum original tau test error\n",
    "    \n",
    "    if avg_loss_per_obs_test_og[t] < min_og_loss:\n",
    "        msd = model.state_dict()\n",
    "        min_og_loss = avg_loss_per_obs_test_og[t]\n",
    "        min_epoch = t+1\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model - at epoch with minimum original tau test error\n",
    "\n",
    "output_path = \"models/density/unif_0_20/len_1000_2000/\"\n",
    "torch.save(msd, output_path + \"epoch_\" + str(min_epoch) + \"_nn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save losses and times for all epochs \n",
    "\n",
    "np.save(output_path + 'train_avgloss.npy', avg_loss_per_obs_train)\n",
    "np.save(output_path + 'test_avgloss.npy', avg_loss_per_obs_test)\n",
    "#np.save(output_path + 'test_norm_avgloss.npy', avg_loss_per_obs_test_norm)\n",
    "#np.save(output_path + 'test_unif_avgloss.npy', avg_loss_per_obs_test_unif)\n",
    "np.save(output_path + 'test_og_avgloss.npy', avg_loss_per_obs_test_og)\n",
    "\n",
    "np.save(output_path + 'train_time.npy', train_time)\n",
    "np.save(output_path + 'test_time.npy', test_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time computations\n",
    "\n",
    "print(f\"Total train time: {sum(train_time)} seconds\")\n",
    "print(f\"Average train epoch time: {np.mean(train_time)} seconds\\n\")\n",
    "\n",
    "print(f\"Total test time: {sum(test_time)} seconds\")\n",
    "print(f\"Average test epoch time: {np.mean(test_time)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.12.0",
   "language": "python",
   "name": "pytorch-1.12.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
